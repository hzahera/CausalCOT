{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "721c9e3d-1255-4782-85d5-c81ad714a7fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# specify the name of dataset: \"hotpotqa_valid_original_split.csv\", \"gooaq_valid_original_split.csv\", or \"msmarco_valid_original_split.csv\"\n",
    "dataset_name= \"msmarco_valid_original_split.csv\"\n",
    "\n",
    "path= \"../../dataset/test/\"+dataset_name\n",
    "\n",
    "out_dir='../../output/Llama2-7B-results/COTs'\n",
    "\n",
    "# name of the fine-tuned model, the pre-trained model is named: \"msmarco-original-split/\", \"hotpotqa-original-split/\" or \"gooaq-original-split/\"\n",
    "model_name= \"TheBloke/Llama-2-7b-Chat-GPTQ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94c0443f-b4c7-425f-8170-7d52ee992f10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Chain-of-thoughts Prompt:\n",
    "\n",
    "COTs_prompt= \"Lets's think step by step to answer the question:\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c0de09-785b-481b-a0b5-d580075d8f58",
   "metadata": {},
   "source": [
    "### Configuration of Llama2-7b Model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74b49572-d0ca-43c4-99f2-e6f1b03a8373",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/upb/users/z/zahera/profiles/unix/cs/.conda/envs/causalQA/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "skip module injection for FusedLlamaMLPForQuantizedModel not support integrate without triton yet.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, pipeline, logging\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "import time\n",
    "\n",
    "model_name_or_path = \"TheBloke/Llama-2-7b-Chat-GPTQ\"\n",
    "model_basename = \"model\"\n",
    "\n",
    "use_triton = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n",
    "        model_basename=model_basename,\n",
    "        use_safetensors=True,\n",
    "        trust_remote_code=True,\n",
    "        device=\"cuda:0\",\n",
    "        skip_special_tokens= True, \n",
    "        use_triton=use_triton,        \n",
    "        quantize_config=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28402728-ada8-4704-ac46-d3e6a78df0a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prevent printing spurious transformers error when using pipeline with AutoGPTQ\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    use_cache=True,\n",
    "    device_map=\"auto\",    \n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    temperature=0.1,\n",
    "    max_length=200,\n",
    "    repetition_penalty=1.1,  \n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7d5f83c-af92-4aed-9ab4-34ff6c03fc42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_model(input_string):\n",
    "    \n",
    "    response=pipe(input_string)        \n",
    "    answer= response[0][\"generated_text\"]\n",
    "    \n",
    "    answer= answer.replace(input_string, '')\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb451d20-4548-463a-9164-0be9be77b644",
   "metadata": {},
   "source": [
    "### Load the dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9a145bb-541f-4b89-b350-222af26b68a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5763f7-cbfc-4794-a176-88b32edc4be5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/upb/users/z/zahera/profiles/unix/cs/.conda/envs/causalQA/lib/python3.10/site-packages/transformers/pipelines/base.py:1090: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataset_df= pd.read_csv(path)\n",
    "\n",
    "dataset_df['generated_answer']=\"\"\n",
    "\n",
    "#iterate over the questions and answers: \n",
    "for row in dataset_df.itertuples():\n",
    "        \n",
    "    # answer the question using the model\n",
    "    generated_answer=run_model(COTs_prompt+row.question_processed)\n",
    "    \n",
    "    dataset_df.loc[row.Index,'generated_answer']= generated_answer\n",
    "\n",
    "    \n",
    "dataset_df.to_csv(out_dir+'/'+dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5c4a94-267d-41fe-8def-e10b8b1f3c9e",
   "metadata": {},
   "source": [
    "### Evaluation using EM and F1 metrics: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf69ef05-0809-444e-a1da-a9d3b2e232d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../src')\n",
    "\n",
    "import measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35649487-6a4d-4466-9711-dcd361b1d663",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " os.chdir(out_dir)\n",
    "\n",
    "def evaluate_unifiedqa(predictions, answers):\n",
    "    \n",
    "    result = {}\n",
    "    result['checkpoint'] = model_name\n",
    "    result['metrics'] = measures.all_metrics(predictions, answers)\n",
    "    result['predictions'] = predictions\n",
    "\n",
    "   \n",
    "    filename=dataset_name.split('.')[0]\n",
    "    filename=filename+\".json\"\n",
    "    \n",
    "    with open(filename, 'w+') as file:\n",
    "        json.dump(result, file, indent=4)\n",
    "        \n",
    "    print ('results saved at ', out_dir+\"/\"+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c9d180d-5c52-4db4-b2e9-2a5c4c48352e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results saved at  ../../output/Llama2-7B-results/pretrained/msmarco_valid_original_split.json\n"
     ]
    }
   ],
   "source": [
    "predictions= dataset_df['generated_answer'].tolist()\n",
    "answers= dataset_df['answer_processed'].tolist()\n",
    "\n",
    "answers= [[answer] for answer in answers]\n",
    "\n",
    "evaluate_unifiedqa(predictions, answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9158cd3f-4f08-4618-9769-328808b0ad45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causalQA",
   "language": "python",
   "name": "causalqa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
