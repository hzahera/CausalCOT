{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59a06ab9-e6d3-4533-9ac9-aa37156e3201",
   "metadata": {},
   "source": [
    "### Specify the datasets: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e7f3c1b-51d1-4f96-a69a-1446f9dd1139",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# specify the name of dataset: \"hotpotqa_valid_original_split.csv\", \"gooaq_valid_original_split.csv\", or \"msmarco_valid_original_split.csv\"\n",
    "dataset_name= \"hotpotqa_valid_original_split.csv\"\n",
    "\n",
    "path= \"../../dataset/test/\"+dataset_name\n",
    "\n",
    "out_dir='../../output/Flan-XXL-results/pretrained'\n",
    "\n",
    "# name of the fine-tuned model, the pre-trained model is named: \"msmarco-original-split/\", \"hotpotqa-original-split/\" or \"gooaq-original-split/\"\n",
    "model_name= \"google/flan-t5-xxl/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3c8759-fa9a-499e-a311-a3332d308a63",
   "metadata": {},
   "source": [
    "### Configuration of FLAN-XXL model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78bb9881-c065-473e-9a30-17d8ebd067bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\n                        Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\n                        the quantized model. If you want to dispatch the model on the CPU or the disk while keeping\n                        these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\n                        `device_map` to `from_pretrained`. Check\n                        https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n                        for more details.\n                        ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m T5Tokenizer, T5ForConditionalGeneration\n\u001b[1;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m T5Tokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/flan-t5-xxl\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mT5ForConditionalGeneration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoogle/flan-t5-xxl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_in_8bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/causalQA/lib/python3.10/site-packages/transformers/modeling_utils.py:3030\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3026\u001b[0m         device_map_without_lm_head \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3027\u001b[0m             key: device_map[key] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m device_map\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m modules_to_not_convert\n\u001b[1;32m   3028\u001b[0m         }\n\u001b[1;32m   3029\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m-> 3030\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3031\u001b[0m \u001b[38;5;250m                \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3032\u001b[0m \u001b[38;5;124;03m                Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\u001b[39;00m\n\u001b[1;32m   3033\u001b[0m \u001b[38;5;124;03m                the quantized model. If you want to dispatch the model on the CPU or the disk while keeping\u001b[39;00m\n\u001b[1;32m   3034\u001b[0m \u001b[38;5;124;03m                these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\u001b[39;00m\n\u001b[1;32m   3035\u001b[0m \u001b[38;5;124;03m                `device_map` to `from_pretrained`. Check\u001b[39;00m\n\u001b[1;32m   3036\u001b[0m \u001b[38;5;124;03m                https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\u001b[39;00m\n\u001b[1;32m   3037\u001b[0m \u001b[38;5;124;03m                for more details.\u001b[39;00m\n\u001b[1;32m   3038\u001b[0m \u001b[38;5;124;03m                \"\"\"\u001b[39;00m\n\u001b[1;32m   3039\u001b[0m             )\n\u001b[1;32m   3040\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m device_map_without_lm_head\n\u001b[1;32m   3042\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: \n                        Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\n                        the quantized model. If you want to dispatch the model on the CPU or the disk while keeping\n                        these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\n                        `device_map` to `from_pretrained`. Check\n                        https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n                        for more details.\n                        "
     ]
    }
   ],
   "source": [
    "# pip install bitsandbytes accelerate\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xxl\", max_length=512)\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xxl\", device_map=\"auto\", load_in_8bit=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4813bb06-d34c-4d40-821d-0e6276bbf7b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_model(input_string, **generator_args):\n",
    "    \n",
    "    input_ids = tokenizer(input_string, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    \n",
    "    res = model.generate(input_ids, max_length=200, **generator_args)\n",
    "    \n",
    "    return tokenizer.batch_decode(res, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c5649c-f89c-4cb8-8bda-35dcdee2b889",
   "metadata": {},
   "source": [
    "### Load the dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dc5eb08-9431-490b-b9f1-4171f3a607c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8621bfc6-c6c5-499f-8d30-e53d6830c83c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_df= pd.read_csv(path)\n",
    "\n",
    "dataset_df['generated_answer']=\"\"\n",
    "\n",
    "#iterate over the questions and answers: \n",
    "for row in dataset_df.itertuples():\n",
    "        \n",
    "    # answer the question using the model\n",
    "    generated_answer=run_model(row.question_processed)\n",
    "    dataset_df.loc[row.Index,'generated_answer']= generated_answer[0]\n",
    "\n",
    "dataset_df.to_csv(out_dir+'/'+dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01e8e13-296e-480d-b1ce-75906c5b3921",
   "metadata": {},
   "source": [
    "### Evaluation using EM and F1 metrics: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa802b88-3135-4281-bd7c-e8f9abe94ab1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../src')\n",
    "\n",
    "import measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b600ea04-c898-4323-99b5-408162e2ad32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " os.chdir(out_dir)\n",
    "\n",
    "def evaluate_unifiedqa(predictions, answers):\n",
    "    \n",
    "    result = {}\n",
    "    result['checkpoint'] = model_name\n",
    "    result['metrics'] = measures.all_metrics(predictions, answers)\n",
    "    result['predictions'] = predictions\n",
    "\n",
    "   \n",
    "    filename=dataset_name.split('.')[0]\n",
    "    filename=filename+\".json\"\n",
    "    \n",
    "    with open(filename, 'w+') as file:\n",
    "        json.dump(result, file, indent=4)\n",
    "        \n",
    "    print ('results saved at ', out_dir+\"/\"+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d94a5235-217e-4672-8aa7-65f5b0b02601",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results saved at  ../../output/Flan-XXL-results/pretrained/gooaq_valid_original_split.json\n"
     ]
    }
   ],
   "source": [
    "predictions= dataset_df['generated_answer'].tolist()\n",
    "answers= dataset_df['answer_processed'].tolist()\n",
    "\n",
    "answers= [[answer] for answer in answers]\n",
    "\n",
    "evaluate_unifiedqa(predictions, answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d05669-e99d-4416-b0e0-12ea321c544d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causalQA",
   "language": "python",
   "name": "causalqa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
