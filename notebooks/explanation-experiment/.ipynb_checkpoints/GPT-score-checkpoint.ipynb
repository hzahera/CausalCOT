{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "540e86e0-2d69-426f-a30c-f56ffd973e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install openai\n",
    "from openai import OpenAI\n",
    "import re\n",
    "\n",
    "# insert your openai key\n",
    "client = OpenAI(api_key = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6276f0-c191-4d78-8dca-618fd20ceb9d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Evaluation prompt for the GPT-4 model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aac6872e-b15c-4f71-aa07-36b98ce578b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_prompt= \"\"\"\n",
    "Your are an expert in evaluating the explanation quality of question answering systems.\n",
    "Your are given a question with its answer, please act as an evaluation metric and evaluate to what extend the [answer]Â is self-explainable, accurate, complete with respect to the [question]. \n",
    "Return the explanation_score as float value in range (0.0 - 1.0).\n",
    "\"\"\"\n",
    "\n",
    "def evaluate_explainability(question, answer): \n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "      model=\"gpt-4\",\n",
    "      temperature=0.0,\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": evaluation_prompt},\n",
    "        {\"role\": \"user\",\"content\": question+answer},\n",
    "        {\"role\": \"user\", \"content\": \"explanation_score = ?\"}\n",
    "      ]\n",
    "    )\n",
    "    \n",
    "    answer=response.choices[0].message.content\n",
    "    \n",
    "    pattern = r'\\d+\\.\\d+' \n",
    "    match = re.search(pattern, answer) \n",
    "\n",
    "    if match: \n",
    "        return float(match.group()) \n",
    "    \n",
    "    else: \n",
    "        print('No floating value found') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45746511-c823-4cdd-b02e-14b8a2e7b427",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load questions from the dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a25dd4e0-5db5-4d22-829f-557bd21fe78f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# dataset: gooaq_valid_original_split.csv, hotpotqa_valid_original_split.csv, msmarco_valid_original_split.csv\n",
    "dataset_name=\"gooaq_valid_original_split.csv\"\n",
    "\n",
    "qa_df= pd.read_csv('../../output/Llama2-7B-results/pretrained/'+dataset_name)\n",
    "\n",
    "qa_df['GPTscore']=''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fea1d29-2883-43a9-834f-23fc77da3afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over the questions and get answers from the Student model\n",
    "for row in qa_df.itertuples():\n",
    "\n",
    "    question=row.question_processed\n",
    "    answer=row.generated_answer\n",
    "                    \n",
    "    # evaluate the explanation score of the student answer before prompt-tuning\n",
    "    output_df.loc[row.Index, 'GPTscore'] = evaluate_explainability(question,answer)    \n",
    "\n",
    "fname=dataset_name[:-4]+\"_explanation.csv\"\n",
    "#saving the results    \n",
    "output_df.to_csv('../../output/Llama2-7B-results/pretrained/'+fname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
